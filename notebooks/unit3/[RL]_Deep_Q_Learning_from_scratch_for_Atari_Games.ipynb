{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOiY2yNQIPWU7Nn7cxacbTp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wengti/Reinforcement-Learning-Tutorial-/blob/main/notebooks/unit3/%5BRL%5D_Deep_Q_Learning_from_scratch_for_Atari_Games.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "collapsed": true,
        "id": "d95GpnGy9VO8",
        "outputId": "c3482677-653e-4787-97d1-b17236ea573d"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "unexpected indent (<ipython-input-5-3953298827>, line 101)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-3953298827>\"\u001b[0;36m, line \u001b[0;32m101\u001b[0m\n\u001b[0;31m    def thunk():\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
          ]
        }
      ],
      "source": [
        "# Source: https://github.com/vwxyzjn/cleanrl/blob/master/cleanrl/dqn_atari.py\n",
        "# docs and experiment results can be found at https://docs.cleanrl.dev/rl-algorithms/dqn/#dqn_ataripy\n",
        "\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import tyro\n",
        "from stable_baselines3.common.atari_wrappers import (\n",
        "    ClipRewardEnv,\n",
        "    EpisodicLifeEnv,\n",
        "    FireResetEnv,\n",
        "    MaxAndSkipEnv,\n",
        "    NoopResetEnv,\n",
        ")\n",
        "from stable_baselines3.common.buffers import ReplayBuffer\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "\n",
        "##########################################\n",
        "# Instantiate a config object (for tyro) #\n",
        "##########################################\n",
        "\n",
        "@dataclass\n",
        "class Args:\n",
        "    exp_name: str = os.path.basename(__file__)[: -len(\".py\")]\n",
        "    \"\"\"the name of this experiment\"\"\"\n",
        "    seed: int = 1\n",
        "    \"\"\"seed of the experiment\"\"\"\n",
        "    torch_deterministic: bool = True\n",
        "    \"\"\"if toggled, `torch.backends.cudnn.deterministic=False`\"\"\"\n",
        "    cuda: bool = True\n",
        "    \"\"\"if toggled, cuda will be enabled by default\"\"\"\n",
        "    track: bool = False\n",
        "    \"\"\"if toggled, this experiment will be tracked with Weights and Biases\"\"\"\n",
        "    wandb_project_name: str = \"cleanRL\"\n",
        "    \"\"\"the wandb's project name\"\"\"\n",
        "    wandb_entity: str = None\n",
        "    \"\"\"the entity (team) of wandb's project\"\"\"\n",
        "    capture_video: bool = False\n",
        "    \"\"\"whether to capture videos of the agent performances (check out `videos` folder)\"\"\"\n",
        "    save_model: bool = False\n",
        "    \"\"\"whether to save model into the `runs/{run_name}` folder\"\"\"\n",
        "    upload_model: bool = False\n",
        "    \"\"\"whether to upload the saved model to huggingface\"\"\"\n",
        "    hf_entity: str = \"\"\n",
        "    \"\"\"the user or org name of the model repository from the Hugging Face Hub\"\"\"\n",
        "\n",
        "    # Algorithm specific arguments\n",
        "    env_id: str = \"BreakoutNoFrameskip-v4\"\n",
        "    \"\"\"the id of the environment\"\"\"\n",
        "    total_timesteps: int = 10000000\n",
        "    \"\"\"total timesteps of the experiments\"\"\"\n",
        "    learning_rate: float = 1e-4\n",
        "    \"\"\"the learning rate of the optimizer\"\"\"\n",
        "    num_envs: int = 1\n",
        "    \"\"\"the number of parallel game environments\"\"\"\n",
        "    buffer_size: int = 1000000\n",
        "    \"\"\"the replay memory buffer size\"\"\"\n",
        "    gamma: float = 0.99\n",
        "    \"\"\"the discount factor gamma\"\"\"\n",
        "    tau: float = 1.0\n",
        "    \"\"\"the target network update rate\"\"\"\n",
        "    target_network_frequency: int = 1000\n",
        "    \"\"\"the timesteps it takes to update the target network\"\"\"\n",
        "    batch_size: int = 32\n",
        "    \"\"\"the batch size of sample from the replay memory\"\"\"\n",
        "    start_e: float = 1\n",
        "    \"\"\"the starting epsilon for exploration\"\"\"\n",
        "    end_e: float = 0.01\n",
        "    \"\"\"the ending epsilon for exploration\"\"\"\n",
        "    exploration_fraction: float = 0.10\n",
        "    \"\"\"the fraction of `total-timesteps` it takes from start-e to go end-e\"\"\"\n",
        "    learning_starts: int = 80000\n",
        "    \"\"\"timestep to start learning\"\"\"\n",
        "    train_frequency: int = 4\n",
        "    \"\"\"the frequency of training\"\"\"\n",
        "\n",
        "\n",
        "def make_env(env_id, seed, idx, capture_video, run_name):\n",
        "  \"\"\"\n",
        "  Make environment\n",
        "\n",
        "  Args:\n",
        "    env_id (str): ID of the environment (refer to the environment documentations). \\n\n",
        "    seed (int): Seed in generating the environment. \\n\n",
        "    idx (int) : Index of the environment. \\n\n",
        "    capture_video (boolean): A flag that decides whether to create an environment that will record episodes. \\n\n",
        "    run_name (str): The name of the folder where the recorded videos will be saved to. \\n\n",
        "\n",
        "  Returns:\n",
        "    thunk (func): A function that returns a created environments.\n",
        "  \"\"\"\n",
        "    def thunk():\n",
        "        # Create an environment with render_mode\n",
        "        # if capture_video is True and only for the first environment\n",
        "        # Record video: Record video intermittently at episode intervals\n",
        "        #(https://gymnasium.farama.org/api/wrappers/misc_wrappers/#gymnasium.wrappers.RecordVideo)\n",
        "        if capture_video and idx == 0:\n",
        "            env = gym.make(env_id, render_mode=\"rgb_array\")\n",
        "            env = gym.wrappers.RecordVideo(env, f\"videos/{run_name}\")\n",
        "        else:\n",
        "            env = gym.make(env_id)\n",
        "\n",
        "        # Enable the environment to keep track of cumulative rewards and episode lengths.\n",
        "        # Save into \"info\" at the end of the episodes\n",
        "        # https://gymnasium.farama.org/api/wrappers/misc_wrappers/#gymnasium.wrappers.RecordEpisodeStatistics\n",
        "        env = gym.wrappers.RecordEpisodeStatistics(env)\n",
        "\n",
        "        # Instead of starting games immediately for each episodes\n",
        "        # The env sample a few random number of \"no-op\" as a way to introduce randomness.\n",
        "        env = NoopResetEnv(env, noop_max=30)\n",
        "\n",
        "        # Return only every skip-th frame (frameskipping)\n",
        "        # And return the max between the two last frames.\n",
        "        # https://stable-baselines3.readthedocs.io/en/master/common/atari_wrappers.html#stable_baselines3.common.atari_wrappers.MaxAndSkipEnv\n",
        "        env = MaxAndSkipEnv(env, skip=4)\n",
        "\n",
        "        # Make end-of-life == end-of-episode, but only reset on true game over.\n",
        "        # https://stable-baselines3.readthedocs.io/en/master/common/atari_wrappers.html#stable_baselines3.common.atari_wrappers.EpisodicLifeEnv.reset\n",
        "        env = EpisodicLifeEnv(env)\n",
        "\n",
        "        # Used for Atari environments that remain static until a \"FIRE\" action is taken.\n",
        "        # Without this, the environment remains static until the agent takes FIRE actions.\n",
        "        # As a result, the agent may spend many timestep at the beginning with a static environment until it fires.\n",
        "        # Which lead to wasting time steps.\n",
        "        # With this, the environment reset as the agent fires. Therefore, the agent are placed in meaningful states without wasting time steps.\n",
        "        # https://stable-baselines3.readthedocs.io/en/master/common/atari_wrappers.html#stable_baselines3.common.atari_wrappers.FireResetEnv\n",
        "        if \"FIRE\" in env.unwrapped.get_action_meanings():\n",
        "            env = FireResetEnv(env)\n",
        "\n",
        "        # Clip the reward to {+1, 0, -1} by its sign.\n",
        "        # Simplifying rewards helps the agent focus on the direction of improvement (good/bad/neutral) rather than the exact magnitude.\n",
        "        # https://stable-baselines3.readthedocs.io/en/master/common/atari_wrappers.html#stable_baselines3.common.atari_wrappers.ClipRewardEnv.reward\n",
        "        env = ClipRewardEnv(env)\n",
        "\n",
        "        # Reduce state information - Resize observations to a smaller size\n",
        "        env = gym.wrappers.ResizeObservation(env, (84, 84))\n",
        "\n",
        "        # Reduce state information - Convert observation into grayscale\n",
        "        env = gym.wrappers.GrayScaleObservation(env)\n",
        "\n",
        "        # Return 4 frame as 1 state - allowing overcoming temporal limitations\n",
        "        # Essentially means that the observation or state that it will be returned in the shape of (4, H, W)\n",
        "        # 4 because each frame is grayscale, and only has a channel of 1\n",
        "        env = gym.wrappers.FrameStack(env, 4)\n",
        "\n",
        "        # Set the seed for the action space\n",
        "        env.action_space.seed(seed)\n",
        "\n",
        "        return env\n",
        "\n",
        "    return thunk\n",
        "\n",
        "\n",
        "# ALGO LOGIC: initialize agent here:\n",
        "class QNetwork(nn.Module):\n",
        "    \"\"\"\n",
        "    A Q-Learning Network.\n",
        "\n",
        "    Args:\n",
        "      env (gym.Env): An environment.\n",
        "    \"\"\"\n",
        "    def __init__(self, env):\n",
        "        super().__init__()\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Conv2d(4, 32, 8, stride=4),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, 4, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, 3, stride=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(3136, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, env.single_action_space.n),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward propagation for the Q-Learning Network.\n",
        "\n",
        "        Args:\n",
        "          x (float tensor): An observation or state, expected in the shape of (B,C,H,W), in the range of 0 - 255\n",
        "\n",
        "        Returns:\n",
        "          out (float tensor): Probability of taking each actions, in the shape of (B, n) where n is the number of unique actions,\n",
        "        \"\"\"\n",
        "        return self.network(x / 255.0)\n",
        "\n",
        "\n",
        "def linear_schedule(start_e: float, end_e: float, duration: int, t: int):\n",
        "    \"\"\"\n",
        "    A linear scheduler that reduces the decaying of epsilon for controlling exploration / exploitation.\n",
        "\n",
        "    Args:\n",
        "      start_e (float): Starting epsilon. \\n\n",
        "      end_e (float): Ending epsilon. \\n\n",
        "      duration (int): Total number of episodes.\n",
        "      t (int): Current number of episodes.\n",
        "\n",
        "    Returns:\n",
        "      epsilon (float): The epsilon for the current number of episode.\n",
        "\n",
        "    \"\"\"\n",
        "    slope = (end_e - start_e) / duration\n",
        "    return max(slope * t + start_e, end_e)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import stable_baselines3 as sb3\n",
        "\n",
        "    if sb3.__version__ < \"2.0\":\n",
        "        raise ValueError(\n",
        "            \"\"\"Ongoing migration: run the following command to install the new dependencies:\n",
        "\n",
        "poetry run pip install \"stable_baselines3==2.0.0a1\" \"gymnasium[atari,accept-rom-license]==0.28.1\"  \"ale-py==0.8.1\"\n",
        "\"\"\"\n",
        "        )\n",
        "\n",
        "    ##############################################\n",
        "    # Obtain the arguments with the help of tyro #\n",
        "    ##############################################\n",
        "    args = tyro.cli(Args)\n",
        "\n",
        "    # Only allow 1 environment\n",
        "    assert args.num_envs == 1, \"vectorized envs are not supported at the moment\"\n",
        "\n",
        "    # Create run names\n",
        "    run_name = f\"{args.env_id}__{args.exp_name}__{args.seed}__{int(time.time())}\"\n",
        "\n",
        "    # Setting up wandb\n",
        "    if args.track:\n",
        "        import wandb\n",
        "\n",
        "        wandb.init(\n",
        "            project=args.wandb_project_name,\n",
        "            entity=args.wandb_entity,\n",
        "            sync_tensorboard=True,\n",
        "            config=vars(args),\n",
        "            name=run_name,\n",
        "            monitor_gym=True,\n",
        "            save_code=True,\n",
        "        )\n",
        "    writer = SummaryWriter(f\"runs/{run_name}\")\n",
        "    writer.add_text(\n",
        "        \"hyperparameters\",\n",
        "        \"|param|value|\\n|-|-|\\n%s\" % (\"\\n\".join([f\"|{key}|{value}|\" for key, value in vars(args).items()])),\n",
        "    )\n",
        "\n",
        "    # TRY NOT TO MODIFY: seeding\n",
        "    random.seed(args.seed)\n",
        "    np.random.seed(args.seed)\n",
        "    torch.manual_seed(args.seed)\n",
        "    torch.backends.cudnn.deterministic = args.torch_deterministic\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() and args.cuda else \"cpu\")\n",
        "\n",
        "    # env setup\n",
        "    # With SyncVectorEnv - the environment reset once truncated or terminated\n",
        "    # This was done under the hood and not need explicitly coded.\n",
        "    envs = gym.vector.SyncVectorEnv(\n",
        "        [make_env(args.env_id, args.seed + i, i, args.capture_video, run_name) for i in range(args.num_envs)]\n",
        "    )\n",
        "\n",
        "    # Check if the action_space are discrete\n",
        "    assert isinstance(envs.single_action_space, gym.spaces.Discrete), \"only discrete action space is supported\"\n",
        "\n",
        "    # Create q_network\n",
        "    q_network = QNetwork(envs).to(device)\n",
        "\n",
        "    # Create optimizer\n",
        "    optimizer = optim.Adam(q_network.parameters(), lr=args.learning_rate)\n",
        "\n",
        "    # Create a copy of the q_network with the same weight and bias\n",
        "    target_network = QNetwork(envs).to(device)\n",
        "    target_network.load_state_dict(q_network.state_dict())\n",
        "\n",
        "    # Create a Replay Buffer\n",
        "    rb = ReplayBuffer(\n",
        "        args.buffer_size,\n",
        "        envs.single_observation_space,\n",
        "        envs.single_action_space,\n",
        "        device,\n",
        "        optimize_memory_usage=True,\n",
        "        handle_timeout_termination=False,\n",
        "    )\n",
        "    start_time = time.time()\n",
        "\n",
        "    # TRY NOT TO MODIFY: start the game\n",
        "    obs, _ = envs.reset(seed=args.seed)\n",
        "    for global_step in range(args.total_timesteps):\n",
        "        # ALGO LOGIC: put action logic here\n",
        "\n",
        "        # Update epsilon\n",
        "        epsilon = linear_schedule(args.start_e, args.end_e, args.exploration_fraction * args.total_timesteps, global_step)\n",
        "\n",
        "        # Sample an action based on epsilon\n",
        "        # Exploration\n",
        "        if random.random() < epsilon:\n",
        "            actions = np.array([envs.single_action_space.sample() for _ in range(envs.num_envs)])\n",
        "\n",
        "        # Exploitation\n",
        "        else:\n",
        "            q_values = q_network(torch.Tensor(obs).to(device))\n",
        "            actions = torch.argmax(q_values, dim=1).cpu().numpy()\n",
        "\n",
        "        # TRY NOT TO MODIFY: execute the game and log data.\n",
        "        # Take a step in that direction\n",
        "        next_obs, rewards, terminations, truncations, infos = envs.step(actions)\n",
        "\n",
        "        # TRY NOT TO MODIFY: record rewards for plotting purposes\n",
        "        # for wandb\n",
        "        if \"final_info\" in infos:\n",
        "            for info in infos[\"final_info\"]:\n",
        "                if info and \"episode\" in info:\n",
        "                    print(f\"global_step={global_step}, episodic_return={info['episode']['r']}\")\n",
        "                    writer.add_scalar(\"charts/episodic_return\", info[\"episode\"][\"r\"], global_step)\n",
        "                    writer.add_scalar(\"charts/episodic_length\", info[\"episode\"][\"l\"], global_step)\n",
        "\n",
        "        # TRY NOT TO MODIFY: save data to reply buffer; handle `final_observation`\n",
        "        real_next_obs = next_obs.copy()\n",
        "\n",
        "        # When the episode ends with truncation, the next_obs will not be the real next observation [I assume its because its reset to the initial state]\n",
        "        # Instead, it will be stored in infos[\"final_observation\"]\n",
        "        # The reason why infos['final_observation'][idx] is used is because it is assuming a vectorised environment\n",
        "\n",
        "        # No need to perform for the termination state\n",
        "        # Because it is still in a meaningful state even after termination (this is before reset)\n",
        "\n",
        "        # In short\n",
        "        # terminated -> in a meaningful state (basically means at the destination) -> then only get reset\n",
        "        # truncated -> in a random state (not really meaningful, like step into a wall) -> then only get reset\n",
        "        # Therefore, only for truncation that we need to find out and replace the true and meaningful next state\n",
        "        for idx, trunc in enumerate(truncations):\n",
        "            if trunc:\n",
        "                real_next_obs[idx] = infos[\"final_observation\"][idx]\n",
        "\n",
        "        # Add the information to the replay buffer\n",
        "        # When adding these information into the buffer,\n",
        "        # For instance if obs has the shape of (num_envs, 4, 32, 32), it will be broken into (4, 32, 32) x num_envs\n",
        "        # Then later on, when called from replay buffer, it will return (Batch_size, 4, 32, 32)\n",
        "        rb.add(obs, real_next_obs, actions, rewards, terminations, infos)\n",
        "\n",
        "        # TRY NOT TO MODIFY: CRUCIAL step easy to overlook\n",
        "        # ignore the case if terminaed or truncated, i am assuming due to SyncVecEnv, it gets reset in other ways later on\n",
        "        obs = next_obs\n",
        "\n",
        "        # ALGO LOGIC: training.\n",
        "        # First check if the number of step is already more than the learning start steps\n",
        "        if global_step > args.learning_starts:\n",
        "\n",
        "            # Check if the step are divisible by the frequency to decide if gradient descent is performed\n",
        "            if global_step % args.train_frequency == 0:\n",
        "\n",
        "                # Sample data up to the batch size\n",
        "                data = rb.sample(args.batch_size)\n",
        "\n",
        "                # Use the target network to find the next actions based on the recorded next observation / state\n",
        "                with torch.no_grad():\n",
        "\n",
        "                    # returns 2 variables because of using .max\n",
        "                    # data.next_observations is in the shape of (B, C, H, W) where B is the number of batch\n",
        "                    # target_network(data.next_observations) return (B, n)\n",
        "                    # target_network(data.next_observations).max(dim=1) returns the target_max (B,) and its corresponding indices (B,)\n",
        "                    target_max, _ = target_network(data.next_observations).max(dim=1)\n",
        "\n",
        "                    # Calculatet the td target\n",
        "                    # td_target = r + gamma * max(target_network(s', a'), dim = a')\n",
        "                    # data.dones is the termination flag\n",
        "                    # if termination - data.dones = 1, hence (1 - data.dones.flatten()) = 1 - 1 = 0\n",
        "                    # There's no need to predict the action in next state\n",
        "                    td_target = data.rewards.flatten() + args.gamma * target_max * (1 - data.dones.flatten())\n",
        "\n",
        "                # Calculate the current old value\n",
        "                # old_val = q_network(s, a)\n",
        "                # Which mean it passes the observation of this sample\n",
        "                # q_network(data.observations) -> Send it to the q_network to get (B, n)\n",
        "                # .gather(1, data.actions) -> Based on the index of of the actions, sample the value in the dim=1 (https://docs.pytorch.org/docs/stable/generated/torch.gather.html)\n",
        "                # .squeeze -> to reduce the shape to (B,)\n",
        "                old_val = q_network(data.observations).gather(1, data.actions).squeeze()\n",
        "\n",
        "                # Compute MSE Loss\n",
        "                loss = F.mse_loss(td_target, old_val)\n",
        "\n",
        "                if global_step % 100 == 0:\n",
        "                    writer.add_scalar(\"losses/td_loss\", loss, global_step)\n",
        "                    writer.add_scalar(\"losses/q_values\", old_val.mean().item(), global_step)\n",
        "                    print(\"SPS:\", int(global_step / (time.time() - start_time)))\n",
        "                    writer.add_scalar(\"charts/SPS\", int(global_step / (time.time() - start_time)), global_step)\n",
        "\n",
        "                # optimize the model\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "            # update target network\n",
        "            # Update partially with the help of tau\n",
        "            if global_step % args.target_network_frequency == 0:\n",
        "                for target_network_param, q_network_param in zip(target_network.parameters(), q_network.parameters()):\n",
        "                    target_network_param.data.copy_(\n",
        "                        args.tau * q_network_param.data + (1.0 - args.tau) * target_network_param.data\n",
        "                    )\n",
        "\n",
        "    if args.save_model:\n",
        "        model_path = f\"runs/{run_name}/{args.exp_name}.cleanrl_model\"\n",
        "        torch.save(q_network.state_dict(), model_path)\n",
        "        print(f\"model saved to {model_path}\")\n",
        "        from cleanrl_utils.evals.dqn_eval import evaluate\n",
        "\n",
        "        episodic_returns = evaluate(\n",
        "            model_path,\n",
        "            make_env,\n",
        "            args.env_id,\n",
        "            eval_episodes=10,\n",
        "            run_name=f\"{run_name}-eval\",\n",
        "            Model=QNetwork,\n",
        "            device=device,\n",
        "            epsilon=0.05,\n",
        "        )\n",
        "        for idx, episodic_return in enumerate(episodic_returns):\n",
        "            writer.add_scalar(\"eval/episodic_return\", episodic_return, idx)\n",
        "\n",
        "        if args.upload_model:\n",
        "            from cleanrl_utils.huggingface import push_to_hub\n",
        "\n",
        "            repo_name = f\"{args.env_id}-{args.exp_name}-seed{args.seed}\"\n",
        "            repo_id = f\"{args.hf_entity}/{repo_name}\" if args.hf_entity else repo_name\n",
        "            push_to_hub(args, episodic_returns, repo_id, \"DQN\", f\"runs/{run_name}\", f\"videos/{run_name}-eval\")\n",
        "\n",
        "    envs.close()\n",
        "    writer.close()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "3"
      ],
      "metadata": {
        "id": "0G7E5h9LW_DX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}